{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5480bcf3-0e3d-47c8-966f-75acdcf20c47",
   "metadata": {},
   "source": [
    "- GAIT ANALYSIS (Optimized Approach with GridSearchCV, No train/test split)\n",
    "\n",
    "- Aim: Multi-class classification to identify subjects (16 individuals) based on gait characteristics, using model comparison and hyperparameter tuning WITHOUT train/test split.\n",
    "\n",
    "- Dataset: As mentioned in notebook named 'gait_final_output_v1_1' \n",
    "\n",
    "- Data Credits: https://archive.ics.uci.edu/dataset/604/gait+classification\n",
    "\n",
    "- Key Improvements:\n",
    "    - No train/test split: All 48 samples used for training with cross-validation for evaluation to maximizes available training data\n",
    "\n",
    "    - Six Models Tested\n",
    "        - Gaussian Naive Bayes: Simple probabilistic classifier (4 param combinations) \n",
    "        - SVC (RBF/Linear): Support Vector Machine with different kernels (12 param combinations)\n",
    "        - Logistic Regression: Linear classifier with regularization (16 param combinations)\n",
    "        - K-Nearest Neighbors: Distance-based classifier (36 param combinations) - BEST\n",
    "        - Random Forest: Ensemble of shallow trees (36 param combinations)\n",
    "        - Decision Tree: Single tree with depth constraints (36 param combinations)\n",
    "\n",
    "    - Hyperparameter Tuning with GridSearchCV\n",
    "        - Systematic search across parameter combinations\n",
    "        - StratifiedKFold cross-validation with n_splits = 3 (min samples/class)\n",
    "        - Scoring: Accuracy metric\n",
    "\n",
    "    - Model Selection for Small Datasets\n",
    "        - Appropriate: K-Nearest Neighbors, SVM, Gaussian Naive Bayes, Logistic Regression\n",
    "        - Poor: Random Forest, XGBoost, Gradient Boosting, Extra Trees \n",
    "\n",
    "- Methodology\n",
    "    - Pipeline Components:\n",
    "        - Oversampling: SMOTEENN or SMOTETomek (with RandomOverSampler fallback)\n",
    "        - Scaling: StandardScaler normalization\n",
    "        - Feature Selection: SelectKBest with mutual information (top ~few features)\n",
    "        - Model: One of six algorithms with tuned hyperparameters\n",
    "        - Validation: StratifiedKFold CV (~3 folds)\n",
    "\n",
    "    - GridSearchCV Configuration: Each model searches through parameter combinations\n",
    "\n",
    "    - Evaluation Strategy:\n",
    "        - Cross-validation score: Performance across 3 folds (best_cv_score)\n",
    "        - Training accuracy: Performance on full dataset (no train/test split)\n",
    "\n",
    "- Results\n",
    "    - KNeighborsClassifier with SMOTETomek consistently achieves highest CV accuracy\n",
    "    - KNN works well because it memorizes local patterns\n",
    "    - With only 3 samples/subject, distance-based classification is effective\n",
    "    - Performance is more stable than complex models\n",
    "   \n",
    "- Performance Variability: \n",
    "    - Results are not fully consistent across runs\n",
    "    - Small dataset leads to high variance\n",
    "    - Different random seeds can produce different \"best\" models\n",
    "    - CV scores provide better estimates than single train/test split\n",
    "      \n",
    "- Limitations:\n",
    "    - Only 3 samples/class is still fundamentally insufficient\n",
    "    - High variance in results despite optimization\n",
    "    - Perfect training accuracy indicates memorization, not learning\n",
    "    - Cross-validation with 3 folds is minimal (each fold has only 1 sample/class)\n",
    "\n",
    "- Summary:\n",
    "    - Using all 48 samples instead of splitting improves data utilization\n",
    "    - GridSearchCV finds better hyperparameters than default values\n",
    "    - Multiple model comparison identifies KNN as most suitable\n",
    "    - Cross-validation provides more reliable evaluation than single test set\n",
    "    - This approach maximizes learning from limited data through: no train/test split (uses all data) => Hyperparameter optimization => Appropriate model selection for small datasets\n",
    "      \n",
    "- Conclusion: See end of this notebook.\n",
    "\n",
    "- Fundamental limitation remains: 48 samples for 16-class classification is insufficient for reliable machine learning, regardless of methodology. Results demonstrate proper workflow but are not scientifically generalizable due to extreme data scarcity. KNeighborsClassifier performs best, but even optimized models cannot overcome the curse of dimensionality with only 3 samples/class.\n",
    "\n",
    "> Recommendations: As mentioned in notebook named 'gait_final_output_v1_1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5dcf0aa1-78d5-4eb5-b62b-bf2945b67692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split, cross_validate, cross_val_score, cross_val_predict\n",
    "from sklearn.model_selection import LeaveOneOut, LeaveOneGroupOut, StratifiedKFold, GroupKFold, GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "#from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "# these clean up the noisy data\n",
    "from imblearn.combine import SMOTETomek, SMOTEENN\n",
    "\n",
    "# these do not clean up the noisy data\n",
    "from imblearn.over_sampling import RandomOverSampler, ADASYN, SMOTE\n",
    "\n",
    "# avoid as it only duplicates data\n",
    "# from imblearn.over_sampling import RandomOverSampler \n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fa664d4-5a6e-41dc-a9b9-5e27dc9ddd63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df.shape: (48, 322)\n",
      "---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Speed_R1</th>\n",
       "      <th>Variability_R1</th>\n",
       "      <th>Symmetry_R1</th>\n",
       "      <th>HeelPressTime_R1</th>\n",
       "      <th>CycleTime_R1</th>\n",
       "      <th>Cadence_R1</th>\n",
       "      <th>Posture_R1</th>\n",
       "      <th>Oscillation_R1</th>\n",
       "      <th>Loading_R1</th>\n",
       "      <th>FootPress_R1</th>\n",
       "      <th>...</th>\n",
       "      <th>P99_R3</th>\n",
       "      <th>P100_R3</th>\n",
       "      <th>P101_R3</th>\n",
       "      <th>P102_R3</th>\n",
       "      <th>P103_R3</th>\n",
       "      <th>P104_R3</th>\n",
       "      <th>P105_R3</th>\n",
       "      <th>P106_R3</th>\n",
       "      <th>P107_R3</th>\n",
       "      <th>Subject_ID_Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1.31</td>\n",
       "      <td>3.48</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.060</td>\n",
       "      <td>1.048</td>\n",
       "      <td>1.050</td>\n",
       "      <td>1.045</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.050</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.1460</td>\n",
       "      <td>0.185</td>\n",
       "      <td>0.222</td>\n",
       "      <td>0.711</td>\n",
       "      <td>1.406</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1.28</td>\n",
       "      <td>4.46</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.158</td>\n",
       "      <td>1.160</td>\n",
       "      <td>1.163</td>\n",
       "      <td>1.157</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.065</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.024</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>0.0129</td>\n",
       "      <td>0.212</td>\n",
       "      <td>0.199</td>\n",
       "      <td>0.658</td>\n",
       "      <td>1.519</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.32</td>\n",
       "      <td>4.15</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.054</td>\n",
       "      <td>1.054</td>\n",
       "      <td>1.050</td>\n",
       "      <td>1.060</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.044</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.0970</td>\n",
       "      <td>0.232</td>\n",
       "      <td>0.215</td>\n",
       "      <td>0.928</td>\n",
       "      <td>1.078</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 322 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Speed_R1  Variability_R1  Symmetry_R1  HeelPressTime_R1  CycleTime_R1  \\\n",
       "33      1.31            3.48          5.1             1.060         1.048   \n",
       "23      1.28            4.46          0.5             1.158         1.160   \n",
       "0       1.32            4.15          4.0             1.054         1.054   \n",
       "\n",
       "    Cadence_R1  Posture_R1  Oscillation_R1  Loading_R1  FootPress_R1  ...  \\\n",
       "33       1.050       1.045           0.120       0.037         0.050  ...   \n",
       "23       1.163       1.157           0.071       0.052         0.065  ...   \n",
       "0        1.050       1.060           0.043       0.044         0.044  ...   \n",
       "\n",
       "    P99_R3  P100_R3  P101_R3  P102_R3  P103_R3  P104_R3  P105_R3  P106_R3  \\\n",
       "33   0.016    0.030    0.020    0.069   0.1460    0.185    0.222    0.711   \n",
       "23   0.015    0.071    0.024   -0.014   0.0129    0.212    0.199    0.658   \n",
       "0    0.027    0.044    0.039    0.073   0.0970    0.232    0.215    0.928   \n",
       "\n",
       "    P107_R3  Subject_ID_Y  \n",
       "33    1.406            11  \n",
       "23    1.519             7  \n",
       "0     1.078             0  \n",
       "\n",
       "[3 rows x 322 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "\n",
    "df = pd.read_csv('gait_final_output.csv')\n",
    "print(f'df.shape: {df.shape}')\n",
    "print(\"---\")\n",
    "df.sample(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d996a47-0137-491f-9bc5-b806e6062e00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Speed_R1            float64\n",
       "Variability_R1      float64\n",
       "Symmetry_R1         float64\n",
       "HeelPressTime_R1    float64\n",
       "CycleTime_R1        float64\n",
       "                     ...   \n",
       "P104_R3             float64\n",
       "P105_R3             float64\n",
       "P106_R3             float64\n",
       "P107_R3             float64\n",
       "Subject_ID_Y          int64\n",
       "Length: 322, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for NA values\n",
    "\n",
    "df.dtypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6bf18c52-1091-4df1-9cde-c42303502142",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for NA values\n",
    "\n",
    "df.isna().sum().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aaf8dc5c-1f33-42e9-84a1-01fbd064f4ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row_index_of_na: 43\n",
      "<class 'numpy.int64'> | int64\n",
      "Index(['CycleTime_R2'], dtype='object')\n",
      "col_name_of_na: CycleTime_R2\n"
     ]
    }
   ],
   "source": [
    "# Get row with na value\n",
    "\n",
    "row_index_of_na = df[ df.isna().any(axis=1) ].index[0]\n",
    "print(\"row_index_of_na:\", row_index_of_na)\n",
    "print( type(row_index_of_na), '|' , row_index_of_na.dtype)\n",
    "\n",
    "\n",
    "print(df.columns[ np.where( df.isna().any(axis=0) == True ) ] )\n",
    "col_name_of_na  = df.columns[  np.where( df.isna().any(axis=0) == True )[0][0]  ]\n",
    "print(\"col_name_of_na:\", col_name_of_na)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "6f44e00a-a807-4df6-86bb-60d35f66666b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute the NA value\n",
    "\n",
    "# Since we have small number of rows, 48, and each 3 rows corresponds to 1 subject, we havae 16 subjects,\n",
    "# the missing values comes from row subject 14's 2nd iteration, and column CycleTime_R2 (R1, R2, R3 are \n",
    "# three different sensors on the body).\n",
    "# Therefore, it is appropriate to take median of CycleTime_R2 for imputing this value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74182f3c-d951-4b47-96a3-430a05282f9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value BEFORE imputation: nan\n",
      "Value AFTER imputation: 6.15\n"
     ]
    }
   ],
   "source": [
    "# median of CycleTime_R2 is \n",
    "\n",
    "print(\"Value BEFORE imputation:\", df.loc[row_index_of_na, col_name_of_na] )\n",
    "\n",
    "median_CycleTime_R2 = df[col_name_of_na].median()\n",
    "\n",
    "df_new = df.copy()\n",
    "    \n",
    "# imput missing value in the df_new\n",
    "df_new.loc[row_index_of_na, col_name_of_na] = median_CycleTime_R2\n",
    "\n",
    "print(\"Value AFTER imputation:\", df_new.loc[row_index_of_na, col_name_of_na] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1bcc9fbc-e993-43f0-b5f4-780b4a712f9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check NA values in the df_new dataframe\n",
    "\n",
    "df_new.isna().sum().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "5e965e6e-a6e3-4ad7-bfaf-ee3e48ba7d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save this \"df_new\" as a new CSV file with name \"gait_final_output_updated.csv\"\n",
    "# df_new.to_csv(\"gait_final_output_updated.csv\", index=False)\n",
    "# Update the data to 'Kaggle' and 'GitHub'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51ea8ff8-0c0f-4681-8663-9cefc44dc58b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
      "Classes are balanced. Max-to-min count ratio is: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Check for class imbalance\n",
    "\n",
    "print( df_new['Subject_ID_Y'].value_counts().to_list() )\n",
    "\n",
    "min_y_count = df_new['Subject_ID_Y'].value_counts().min()\n",
    "max_y_count = df_new['Subject_ID_Y'].value_counts().max()\n",
    "\n",
    "if min_y_count/max_y_count > 5: \n",
    "    print(f\"Classes are imbalanced. Max-to-min count ratio is: {min_y_count/max_y_count}\")\n",
    "else:\n",
    "    print(f\"Classes are balanced. Max-to-min count ratio is: {min_y_count/max_y_count}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c8688c6-9e33-4387-b2a4-bf00c200d239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 48\n",
      "Total features: 321\n",
      "Number of classes: 16\n",
      "Class distribution in full dataset:\n",
      "[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n"
     ]
    }
   ],
   "source": [
    "# STEP 0: Split X and y\n",
    "\n",
    "y =  df_new['Subject_ID_Y']\n",
    "X =  df_new.drop('Subject_ID_Y', axis='columns')\n",
    "\n",
    "print(f\"Total samples: {len(y)}\")\n",
    "print(f\"Total features: {X.shape[1]}\")\n",
    "print(f\"Number of classes: {len(y.unique())}\")\n",
    "print(f\"Class distribution in full dataset:\")\n",
    "print(y.value_counts().sort_index().to_list())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "57816996-b369-4ac0-acea-2286671e5976",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(10, X.shape[1], len(X) // 10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "ef03859e-9489-44b9-9a01-33575f764646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== smoteenn ===\n",
      "k_neighbors: 2\n",
      "y.value_counts().min(): 3\n",
      "Only 3 samples per class - using RandomOverSampler instead of smoteenn\n",
      "oversampler used:  RandomOverSampler(random_state=42)\n",
      "---\n",
      "-------end of model for loop iter---------\n",
      "---\n",
      "-------end of model for loop iter---------\n",
      "---\n",
      "-------end of model for loop iter---------\n",
      "---\n",
      "-------end of model for loop iter---------\n",
      "---\n",
      "-------end of model for loop iter---------\n",
      "---\n",
      "-------end of model for loop iter---------\n",
      "-------end of sampler for loop iter---------\n",
      "\n",
      "=== smotetomek ===\n",
      "k_neighbors: 2\n",
      "y.value_counts().min(): 3\n",
      "Only 3 samples per class - using RandomOverSampler instead of smotetomek\n",
      "oversampler used:  RandomOverSampler(random_state=42)\n",
      "---\n",
      "-------end of model for loop iter---------\n",
      "---\n",
      "-------end of model for loop iter---------\n",
      "---\n",
      "-------end of model for loop iter---------\n",
      "---\n",
      "-------end of model for loop iter---------\n",
      "---\n",
      "-------end of model for loop iter---------\n",
      "---\n",
      "-------end of model for loop iter---------\n",
      "-------end of sampler for loop iter---------\n",
      "---------end of execution---------\n"
     ]
    }
   ],
   "source": [
    "# Define oversampler TYPES\n",
    "oversampler_types = ['smoteenn', 'smotetomek']\n",
    "oversampler_count = len(oversampler_types)\n",
    "\n",
    "results = {}\n",
    "\n",
    "for key in oversampler_types:\n",
    "    print(f'\\n=== {key} ===')\n",
    "    results[key] = {}\n",
    "    \n",
    "    # Train-test split: NO NO NO\n",
    "    # X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.33, random_state=42 )\n",
    "    # print(\"y_train distribution:\", y_train.value_counts().sort_index().to_list())\n",
    "    \n",
    "    # Get sample counts in train data\n",
    "    n_samples = y.value_counts().min()\n",
    "    k_neighbors = max(1, min(3, n_samples - 1))\n",
    "    print(\"k_neighbors:\", k_neighbors)\n",
    "    print(\"y.value_counts().min():\", n_samples)\n",
    "    \n",
    "    # Choose appropriate oversampler based on sample size\n",
    "    if n_samples < 6:  # Too few samples for SMOTEENN/SMOTETomek\n",
    "        print(f\"Only {n_samples} samples per class - using RandomOverSampler instead of {key}\")\n",
    "        oversampler = RandomOverSampler(random_state=42)\n",
    "    else:\n",
    "        sampling_strategy = {cls: target_samples_per_class for cls in np.unique(y)}\n",
    "        \n",
    "        if key == 'smoteenn':\n",
    "            oversampler = SMOTEENN(\n",
    "                smote=SMOTE(random_state=42, sampling_strategy=sampling_strategy, k_neighbors=k_neighbors),\n",
    "                random_state=42,\n",
    "                n_jobs=-1\n",
    "            )\n",
    "        elif key == 'smotetomek':\n",
    "            oversampler = SMOTETomek(\n",
    "                smote=SMOTE(random_state=42, sampling_strategy=sampling_strategy, k_neighbors=k_neighbors),\n",
    "                random_state=42,\n",
    "                n_jobs=-1\n",
    "            )\n",
    "\n",
    "    print(\"oversampler used: \", oversampler)\n",
    "\n",
    "    # Now try differnt models in the pipeline, not just one model.\n",
    "    models = {\n",
    "        'GaussianNB': {\n",
    "            'model': GaussianNB(),\n",
    "            'params': {                \n",
    "                'var_smoothing': np.logspace(-10, -7, num=4) \n",
    "            }\n",
    "        },\n",
    "        'SVC-RBF': {\n",
    "            'model': SVC(random_state=42, decision_function_shape='ovr'),\n",
    "            'params': {\n",
    "                'C':      [0.1, 1, 10],\n",
    "                'gamma':  ['scale', 0.1],\n",
    "                'kernel': ['rbf', 'linear']\n",
    "            }\n",
    "        },    \n",
    "       'Logistic': {\n",
    "            'model': LogisticRegression(max_iter=1000, random_state=42, multi_class='ovr'),\n",
    "            'params': {\n",
    "                'C':       [0.01, 0.1, 1, 10],\n",
    "                'penalty': ['l1', 'l2'],\n",
    "                'solver':  ['liblinear', 'saga']\n",
    "                }\n",
    "        },\n",
    "        'KNeighborsClassifier': {\n",
    "            'model': KNeighborsClassifier(),\n",
    "            'params': {\n",
    "                'n_neighbors':[1, 2, 3],\n",
    "                'weights':\t  ['uniform', 'distance'],\n",
    "                'metric':\t  ['euclidean', 'manhattan', 'minkowski'],\n",
    "                'p':          [1, 2]\n",
    "                }\n",
    "        },\n",
    "        'RandomForest': {\n",
    "            'model': RandomForestClassifier(random_state=42, n_jobs=-1),\n",
    "            'params': {\n",
    "                'n_estimators':      [50, 100],        # Not too many\n",
    "                'max_depth':         [2, 3, 4],        # VERY shallow (key!)\n",
    "                'min_samples_split': [2, 3, 5],        # Require multiple samples\n",
    "                'min_samples_leaf':  [1, 2]            # Prevent tiny leaves\n",
    "                }\n",
    "        },\n",
    "        'DecisionTree': {\n",
    "            'model': DecisionTreeClassifier(random_state=42),\n",
    "            'params': {\n",
    "                'max_depth':         [2, 3, 4, 5],\n",
    "                'min_samples_split': [2, 3, 5],\n",
    "                'min_samples_leaf':  [1, 2, 3]\n",
    "                }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    \n",
    "    # Check all model's performance\n",
    "    for models_name, models_config in models.items():\n",
    "        print('---')\n",
    "        model_start_time = time.time()\n",
    "                    \n",
    "        # Pipeline        \n",
    "        pipeline = Pipeline([\n",
    "            ('oversample', oversampler),\n",
    "            ('scaler',     StandardScaler()),\n",
    "            ('selector',   SelectKBest(mutual_info_classif, k= min(10, X.shape[1], len(X) // 10) )),\n",
    "            ('model',      models_config['model'] )\n",
    "        ])\n",
    "        param_grid = models_config['params']\n",
    "        \n",
    "        #print(\"param_grid:\", param_grid) \n",
    "        #print( {f'model__{k}': v for k, v in models_config['params'].items()} )\n",
    "        param_grid = {f'model__{k}':v for k,v in models_config['params'].items() } \n",
    "        \n",
    "        # GridSearchCV \n",
    "        cv = StratifiedKFold(n_splits=y.value_counts().min(), shuffle=True, random_state=42)\n",
    "        # n_splits=3 cannot be greater than the number of members in each class.\n",
    "        try:\n",
    "            grid_search = GridSearchCV(\n",
    "                estimator  = pipeline,\n",
    "                param_grid = param_grid,\n",
    "                cv         = cv,\n",
    "                scoring    = 'accuracy', # can explore this later for precision/recall\n",
    "                n_jobs     = -1,\n",
    "                verbose    = 0\n",
    "            )\n",
    "            grid_search.fit(X, y)\n",
    "\n",
    "            # evaluate\n",
    "            train_acc = grid_search.score(X, y)\n",
    "            #test_acc  = grid_search.score(X_test, y_test)\n",
    "\n",
    "            results[key][models_name] = {\n",
    "                'grid'          : grid_search,\n",
    "                'train_acc'     : train_acc,\n",
    "                #'test_acc'      : test_acc,\n",
    "                'best_param'    : grid_search.best_params_,\n",
    "                'best_estimator': grid_search.best_estimator_,\n",
    "                'best_cv_score' : grid_search.best_score_,\n",
    "                'model_run_time': time.time() - model_start_time\n",
    "            }\n",
    "            \n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"{models_name} ERROR: {str(e)}\")\n",
    "            continue\n",
    "            \n",
    "        print(\"-------end of model for loop iter---------\")\n",
    "    print(\"-------end of sampler for loop iter---------\")\n",
    "print(\"---------end of execution---------\")\n",
    "    \n",
    "# Print results after the execution ends\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "2afe20b9-d594-471d-a11f-9090788eed9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       sampler                 model  train_acc  best_cv_score  model_run_time\n",
      "9   smotetomek  KNeighborsClassifier       1.00           0.92          326.14\n",
      "1     smoteenn               SVC-RBF       0.94           0.85          120.59\n",
      "7   smotetomek               SVC-RBF       0.92           0.85          114.52\n",
      "2     smoteenn              Logistic       0.88           0.83          165.67\n",
      "3     smoteenn  KNeighborsClassifier       1.00           0.83          324.41\n",
      "8   smotetomek              Logistic       0.88           0.81          151.37\n",
      "4     smoteenn          RandomForest       0.96           0.79          351.13\n",
      "10  smotetomek          RandomForest       0.96           0.79          349.63\n",
      "6   smotetomek            GaussianNB       0.90           0.52           43.45\n",
      "0     smoteenn            GaussianNB       0.96           0.46           48.17\n",
      "5     smoteenn          DecisionTree       0.44           0.40          334.93\n",
      "11  smotetomek          DecisionTree       0.44           0.40          322.95\n"
     ]
    }
   ],
   "source": [
    "# print a nice looking table\n",
    "data = []\n",
    "for keys, vals in results.items():\n",
    "    for key, val in vals.items():\n",
    "        data.append({\n",
    "            'sampler':       keys,\n",
    "            'model'  :       key,\n",
    "            'train_acc':     round(val['train_acc'],2),\n",
    "            'best_cv_score': round(val['best_cv_score'],2),\n",
    "            'model_run_time':round(val['model_run_time'],2)\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(data).sort_values(by='best_cv_score', ascending=False)\n",
    "print( df )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "83226a10-7fca-45ac-b2c5-b49d03cac75f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The above performance is not consistent. However, KNeighborsClassifier with smotetomek is consistently the highest performer.\n"
     ]
    }
   ],
   "source": [
    "print(\"The above performance is not consistent. \\\n",
    "However, KNeighborsClassifier with smotetomek \\\n",
    "is consistently the highest performer.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "4c3b438a-3986-4c19-8bba-206facae0211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "Best param:  {'model__metric': 'manhattan', 'model__n_neighbors': 2, 'model__p': 2, 'model__weights': 'distance'}\n",
      "------------------------------------------------------------\n",
      "\n",
      "Best estimator:  Pipeline(steps=[('oversample', RandomOverSampler(random_state=42)),\n",
      "                ('scaler', StandardScaler()),\n",
      "                ('selector',\n",
      "                 SelectKBest(k=4,\n",
      "                             score_func=<function mutual_info_classif at 0x71503f173a30>)),\n",
      "                ('model',\n",
      "                 KNeighborsClassifier(metric='manhattan', n_neighbors=2,\n",
      "                                      weights='distance'))])\n",
      "------------------------------------------------------------\n",
      "\n",
      "Best params from results: {'model__metric': 'manhattan', 'model__n_neighbors': 2, 'model__p': 2, 'model__weights': 'distance'}\n",
      "------------------------------------------------------------\n",
      "\n",
      "Best params from estimator: {'model__metric': 'manhattan', 'model__n_neighbors': 2, 'model__p': 2, 'model__weights': 'distance'}\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Get the first value in the table\n",
    "first_sampler = df['sampler'].iloc[0]\n",
    "first_model   = df['model'].iloc[0]\n",
    "\n",
    "# Get the best estimator and params from results\n",
    "estimator   = results[first_sampler][first_model]['best_estimator']\n",
    "model_param = results[first_sampler][first_model]['best_param']\n",
    "\n",
    "print('-'*60)\n",
    "print(\"Best param: \", model_param)\n",
    "print('-'*60)\n",
    "print(\"\\nBest estimator: \", estimator)\n",
    "print('-'*60)\n",
    "\n",
    "# Verify they match by extracting params from estimator\n",
    "best_estimator_params = estimator.get_params()\n",
    "tuned_params = {k: v for k, v in best_estimator_params.items() if k in model_param}\n",
    "\n",
    "print(\"\\nBest params from results:\", model_param)\n",
    "print('-'*60)\n",
    "print(\"\\nBest params from estimator:\", tuned_params)\n",
    "print('-'*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99a4577-b2fa-4bc5-b23b-799d76669c6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "b006b833-cd32-4e25-9767-7809432b53dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean CV Accuracy: 0.8125\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         3\n",
      "           1       1.00      1.00      1.00         3\n",
      "           2       1.00      1.00      1.00         3\n",
      "           3       1.00      1.00      1.00         3\n",
      "           4       1.00      1.00      1.00         3\n",
      "           5       1.00      1.00      1.00         3\n",
      "           6       1.00      1.00      1.00         3\n",
      "           7       1.00      1.00      1.00         3\n",
      "           8       1.00      1.00      1.00         3\n",
      "           9       1.00      1.00      1.00         3\n",
      "          10       1.00      1.00      1.00         3\n",
      "          11       1.00      1.00      1.00         3\n",
      "          12       1.00      1.00      1.00         3\n",
      "          13       1.00      1.00      1.00         3\n",
      "          14       1.00      1.00      1.00         3\n",
      "          15       1.00      1.00      1.00         3\n",
      "\n",
      "    accuracy                           1.00        48\n",
      "   macro avg       1.00      1.00      1.00        48\n",
      "weighted avg       1.00      1.00      1.00        48\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the best model/params to see if I get the same result\n",
    "\n",
    "# Use the best_estimator directly\n",
    "best_pipeline = results[first_sampler][first_model]['best_estimator']\n",
    "\n",
    "# Fit on your training data & predict output\n",
    "best_pipeline.fit(X, y)\n",
    "y_pred = best_pipeline.predict(X)\n",
    "\n",
    "# Evaluate\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "cv_accuracy = cross_val_score(\n",
    "    estimator = best_pipeline, \n",
    "    X=X,\n",
    "    y=y, \n",
    "    scoring='accuracy',\n",
    "    cv=cv\n",
    ")\n",
    "print('Mean CV Accuracy:', cv_accuracy.mean())\n",
    "\n",
    "report = classification_report(y, y_pred)\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e3eaf0-39aa-4101-a3bb-9aafe276b795",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "57be86fe-7f9e-4bd0-949d-43786d2df9e9",
   "metadata": {},
   "source": [
    "SUMMARY:\n",
    "- The best performing model is KNeighborsClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103032aa-61ed-45a1-b029-04f2d69c5df0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
