{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5480bcf3-0e3d-47c8-966f-75acdcf20c47",
   "metadata": {},
   "source": [
    "Gait Analysis (baseline approach)\n",
    "\n",
    "Aim: Multi-class classification (16 subjects) using train/test split with SMOTE-based oversampling in pipeline - the first attempt/baseline approach.\n",
    "\n",
    "Data Source/Credit: https://archive.ics.uci.edu/dataset/604/gait+classification\n",
    "\n",
    "Dataset\n",
    "- Size:         48 samples (16 subjects * 3 trials)\n",
    "- Features:     321 gait cycle measurements from 3 sensors (R1, R2, R3)\n",
    "- Target:       Subject identification (16 classes)\n",
    "- Missing data: 1 NA value in CycleTime_R2 (imputed with median)\n",
    "- Split:        67% train / 33% test -> ~2 training samples/class + 1 test sample/class \n",
    "\n",
    "Methodology: Standard supervised learning pipeline:\n",
    "- Data Cleaning: Median imputation for missing value\n",
    "- Stratified Split: 67/33 train/test (ensures all classes represented)\n",
    "- Oversampling (in pipeline): SMOTEENN or SMOTETomek\n",
    "- Feature Scaling: StandardScaler normalization\n",
    "- Feature Selection: SelectKBest with mutual information (top ~10 features)\n",
    "- Model: Gaussian Naive Bayes (fixed hyperparameters)\n",
    "- Validation: Adaptive cross-validation (skipped when n<3 samples/class)\n",
    "\n",
    "Oversampling strategy\n",
    "- SMOTEENN: SMOTE + Edited Nearest Neighbors (removes noisy synthetic samples)\n",
    "- SMOTETomek: SMOTE + Tomek Links (cleans class boundaries)\n",
    "- Fallback: RandomOverSampler when n_samples < 6 (too few for SMOTE)\n",
    "\n",
    "Adaptive CV: Smart handling of tiny dataset\n",
    "- If n_samples < 3: Skip CV, just train and evaluate on test\n",
    "- If n_samples >= 3: Use StratifiedKFold with n_splits = min(5, n_samples)\n",
    "\n",
    "Results: Very poor performance \n",
    "- 0.00 recall/precision for many classes\n",
    "- Model never predicts certain classes\n",
    "- High variance, no generalization\n",
    "\n",
    "Why This Failed\n",
    "1. Insufficient training data: only 2 training samples per class:\n",
    "    - Impossible to learn distinguishing patterns\n",
    "    - Model cannot identify what makes each subject unique\n",
    "    - Not enough examples to capture gait variability\n",
    "\n",
    "2. Unreliable evaluation: only 1 test sample per class:\n",
    "    - Single misclassification = 100% error for that subject\n",
    "    - Metrics are completely unstable\n",
    "    - Cannot distinguish signal from noise\n",
    "\n",
    "3. Oversampling limitations: SMOTE/SMOTEENN/SMOTETomek with 2 samples:\n",
    "    - Creates synthetic samples by interpolating between only 2 points\n",
    "    - Very limited diversity in synthetic data\n",
    "    - Essentially creates variations of the same 2 examples\n",
    "    - Cannot add meaningful new information\n",
    "\n",
    "4. Curse of Dimensionality\n",
    "    - 321 features vs 32 training samples\n",
    "    - Features >> Samples (severe overfitting risk)\n",
    "    - Even after feature selection (10 features), ratio still problematic\n",
    "    - Model cannot learn reliable patterns\n",
    "\n",
    "What This Baseline Established\n",
    "1. Lessons Learned\n",
    "    - Proper methodology: No data leakage (oversampling in pipeline after split)\n",
    "    - Adaptive approach: Handles edge cases (n_samples < 3)\n",
    "    - Smart oversampling: Tests multiple methods with fallback\n",
    "    - Feature selection: Reduces dimensionality\n",
    "\n",
    "2. Fundamental Problems Identified\n",
    "    - Dataset too small: 48 samples insufficient for 16-class problem\n",
    "    - Train/test split wasteful: Loses 16 samples to test set\n",
    "    - Single split unstable: Results vary drastically with random_state\n",
    "    - Not person-independent: Tests on subjects seen during training\n",
    "\n",
    "3. Comparison with later versions, what v1_1 did right\n",
    "    - Correct pipeline implementation (no data leakage)\n",
    "    - Stratified split preserves class balance\n",
    "    - Adaptive handling of tiny samples\n",
    "    - Tests multiple oversampling methods\n",
    "\n",
    "What later versions improved \n",
    "- v1_2: Added GridSearchCV for hyperparameter tuning, tested multiple models (KNN, SVC, Logistic, RandomForest, DecisionTree), no train/test split (uses all 48 samples)\n",
    "- v1_3: Tested aggressive oversampling (100 samples/class) to see if more synthetic data helps - showed it doesn't (BAD RESULT DUE TO OVERFITTING)\n",
    "- v1_4: Demonstrated data leakage by oversampling BEFORE split - educational example of what NOT to do (VERY BAD)\n",
    "- v1_5: Tested oversampling outside pipeline (**UPDATE**)\n",
    "- v1_6: Tested LOOCV with pre-CV oversampling (480 folds) - showed 99.4% accuracy but with data leakage (Invalid - educational)\n",
    "- v1_7: Removed train/test split, used StratifiedKFold (2-fold) with pipeline oversampling, tested multiple k values (2,3,4,5,10,15) - BEST APPROACH\n",
    "- v1_8: Same as v1_7 but with train/test split added back, showed why split is inferior for tiny data (88.2% accuracy, unstable)\n",
    "\n",
    "What v1_1 did right\n",
    "- Correct pipeline implementation (no data leakage)\n",
    "- Stratified split preserves class balance\n",
    "- Adaptive handling of tiny samples\n",
    "- Tests multiple oversampling methods\n",
    "\n",
    "What v1_1 did that others avoided\n",
    "- Train/test split: Later versions realized this wastes data with only 48 samples\n",
    "- Fixed hyperparameters: v1_2 added systematic tuning\n",
    "- Single model: Later versions compared multiple algorithms\n",
    "\n",
    "Valid conclusions\n",
    "- Data scarcity is the problem: No amount of clever methodology can overcome 2 samples/class\n",
    "- Oversampling has limits: Cannot create meaningful diversity from 2 examples\n",
    "- Need different approach: Train/test split inappropriate for this dataset size\n",
    "\n",
    "Invalid conclusions\n",
    "- Cannot conclude model quality (data too small to evaluate)\n",
    "- Cannot conclude feature quality (not enough samples to test)\n",
    "- Cannot conclude biometric viability (not person-independent anyway)\n",
    "\n",
    "Other possible solutions\n",
    "- Collect more data: 30-50 samples/subject minimum\n",
    "- Simplify problem: Binary classification instead of 16-class\n",
    "- Alternative approaches: Unsupervised learning, one-vs-rest\n",
    "- Use GroupKFold: Test person-independent performance\n",
    " \n",
    "This baseline established:\n",
    "- What proper methodology looks like (no leakage, stratified splits)\n",
    "- That the dataset is too small for reliable classification\n",
    "- Need for alternative approaches (led to v1_2, v1_7 improvements)\n",
    "\n",
    "> This was the starting point that established both the methodology and the fundamental challenge of working with extremely small datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5dcf0aa1-78d5-4eb5-b62b-bf2945b67692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split, cross_validate, cross_val_score, cross_val_predict\n",
    "from sklearn.model_selection import LeaveOneOut, LeaveOneGroupOut, StratifiedKFold, GroupKFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "#from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "# these clean up the noisy data\n",
    "from imblearn.combine import SMOTETomek, SMOTEENN\n",
    "\n",
    "# these do not clean up the noisy data\n",
    "from imblearn.over_sampling import RandomOverSampler, ADASYN, SMOTE\n",
    "\n",
    "# avoid as it only duplicates data\n",
    "# from imblearn.over_sampling import RandomOverSampler \n",
    "\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fa664d4-5a6e-41dc-a9b9-5e27dc9ddd63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df.shape: (48, 322)\n",
      "---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Speed_R1</th>\n",
       "      <th>Variability_R1</th>\n",
       "      <th>Symmetry_R1</th>\n",
       "      <th>HeelPressTime_R1</th>\n",
       "      <th>CycleTime_R1</th>\n",
       "      <th>Cadence_R1</th>\n",
       "      <th>Posture_R1</th>\n",
       "      <th>Oscillation_R1</th>\n",
       "      <th>Loading_R1</th>\n",
       "      <th>FootPress_R1</th>\n",
       "      <th>...</th>\n",
       "      <th>P99_R3</th>\n",
       "      <th>P100_R3</th>\n",
       "      <th>P101_R3</th>\n",
       "      <th>P102_R3</th>\n",
       "      <th>P103_R3</th>\n",
       "      <th>P104_R3</th>\n",
       "      <th>P105_R3</th>\n",
       "      <th>P106_R3</th>\n",
       "      <th>P107_R3</th>\n",
       "      <th>Subject_ID_Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.27</td>\n",
       "      <td>4.12</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.113</td>\n",
       "      <td>1.112</td>\n",
       "      <td>1.115</td>\n",
       "      <td>1.115</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.040</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.130</td>\n",
       "      <td>0.128</td>\n",
       "      <td>0.216</td>\n",
       "      <td>0.216</td>\n",
       "      <td>1.010</td>\n",
       "      <td>0.990</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>1.39</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-5.3</td>\n",
       "      <td>1.162</td>\n",
       "      <td>1.161</td>\n",
       "      <td>1.078</td>\n",
       "      <td>1.070</td>\n",
       "      <td>0.702</td>\n",
       "      <td>0.687</td>\n",
       "      <td>0.035</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.168</td>\n",
       "      <td>0.122</td>\n",
       "      <td>0.238</td>\n",
       "      <td>0.239</td>\n",
       "      <td>1.007</td>\n",
       "      <td>0.993</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>1.42</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-8.1</td>\n",
       "      <td>1.123</td>\n",
       "      <td>1.190</td>\n",
       "      <td>1.125</td>\n",
       "      <td>1.125</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.603</td>\n",
       "      <td>0.031</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.171</td>\n",
       "      <td>0.235</td>\n",
       "      <td>0.249</td>\n",
       "      <td>0.930</td>\n",
       "      <td>1.075</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 322 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Speed_R1  Variability_R1  Symmetry_R1  HeelPressTime_R1  CycleTime_R1  \\\n",
       "20      1.27            4.12          1.8             1.113         1.112   \n",
       "40      1.39            0.00         -5.3             1.162         1.161   \n",
       "47      1.42            0.00         -8.1             1.123         1.190   \n",
       "\n",
       "    Cadence_R1  Posture_R1  Oscillation_R1  Loading_R1  FootPress_R1  ...  \\\n",
       "20       1.115       1.115           0.044       0.046         0.040  ...   \n",
       "40       1.078       1.070           0.702       0.687         0.035  ...   \n",
       "47       1.125       1.125           0.040       0.603         0.031  ...   \n",
       "\n",
       "    P99_R3  P100_R3  P101_R3  P102_R3  P103_R3  P104_R3  P105_R3  P106_R3  \\\n",
       "20   0.020    0.026    0.027    0.130    0.128    0.216    0.216    1.010   \n",
       "40   0.018    0.017    0.015    0.168    0.122    0.238    0.239    1.007   \n",
       "47   0.017    0.024    0.017    0.143    0.171    0.235    0.249    0.930   \n",
       "\n",
       "    P107_R3  Subject_ID_Y  \n",
       "20    0.990             6  \n",
       "40    0.993            13  \n",
       "47    1.075            15  \n",
       "\n",
       "[3 rows x 322 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "\n",
    "df = pd.read_csv('gait_final_output.csv')\n",
    "print(f'df.shape: {df.shape}')\n",
    "print(\"---\")\n",
    "df.sample(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d996a47-0137-491f-9bc5-b806e6062e00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Speed_R1            float64\n",
       "Variability_R1      float64\n",
       "Symmetry_R1         float64\n",
       "HeelPressTime_R1    float64\n",
       "CycleTime_R1        float64\n",
       "                     ...   \n",
       "P104_R3             float64\n",
       "P105_R3             float64\n",
       "P106_R3             float64\n",
       "P107_R3             float64\n",
       "Subject_ID_Y          int64\n",
       "Length: 322, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for NA values\n",
    "\n",
    "df.dtypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6bf18c52-1091-4df1-9cde-c42303502142",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for NA values\n",
    "\n",
    "df.isna().sum().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aaf8dc5c-1f33-42e9-84a1-01fbd064f4ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row_index_of_na: 43\n",
      "<class 'numpy.int64'> | int64\n",
      "Index(['CycleTime_R2'], dtype='object')\n",
      "col_name_of_na: CycleTime_R2\n"
     ]
    }
   ],
   "source": [
    "# Get row with na value\n",
    "\n",
    "row_index_of_na = df[ df.isna().any(axis=1) ].index[0]\n",
    "print(\"row_index_of_na:\", row_index_of_na)\n",
    "print( type(row_index_of_na), '|' , row_index_of_na.dtype)\n",
    "\n",
    "\n",
    "print(df.columns[ np.where( df.isna().any(axis=0) == True ) ] )\n",
    "col_name_of_na  = df.columns[  np.where( df.isna().any(axis=0) == True )[0][0]  ]\n",
    "print(\"col_name_of_na:\", col_name_of_na)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f44e00a-a807-4df6-86bb-60d35f66666b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute the NA value\n",
    "\n",
    "# Since we have small number of rows, 48, and each 3 rows corresponds to 1 subject, we havae 16 subjects,\n",
    "# the missing values comes from row subject 14's 2nd iteration, and column CycleTime_R2 (R1, R2, R3 are \n",
    "# three different sensors on the body).\n",
    "# Therefore, it is appropriate to take median of CycleTime_R2 for imputing this value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74182f3c-d951-4b47-96a3-430a05282f9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value BEFORE imputation: nan\n",
      "Value AFTER imputation: 6.15\n"
     ]
    }
   ],
   "source": [
    "# median of CycleTime_R2 is \n",
    "\n",
    "print(\"Value BEFORE imputation:\", df.loc[row_index_of_na, col_name_of_na] )\n",
    "\n",
    "median_CycleTime_R2 = df[col_name_of_na].median()\n",
    "\n",
    "df_new = df.copy()\n",
    "    \n",
    "# imput missing value in the df_new\n",
    "df_new.loc[row_index_of_na, col_name_of_na] = median_CycleTime_R2\n",
    "\n",
    "print(\"Value AFTER imputation:\", df_new.loc[row_index_of_na, col_name_of_na] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1bcc9fbc-e993-43f0-b5f4-780b4a712f9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check NA values in the df_new dataframe\n",
    "\n",
    "df_new.isna().sum().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e965e6e-a6e3-4ad7-bfaf-ee3e48ba7d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save this \"df_new\" as a new CSV file with name \"gait_final_output_updated.csv\"\n",
    "# df_new.to_csv(\"gait_final_output_updated.csv\", index=False)\n",
    "# Update the data to 'Kaggle' and 'GitHub'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51ea8ff8-0c0f-4681-8663-9cefc44dc58b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
      "Classes are balanced. Max-to-min count ratio is: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Check for class imbalance\n",
    "\n",
    "print( df_new['Subject_ID_Y'].value_counts().to_list() )\n",
    "\n",
    "min_y_count = df_new['Subject_ID_Y'].value_counts().min()\n",
    "max_y_count = df_new['Subject_ID_Y'].value_counts().max()\n",
    "\n",
    "if min_y_count/max_y_count > 5: \n",
    "    print(f\"Classes are imbalanced. Max-to-min count ratio is: {min_y_count/max_y_count}\")\n",
    "else:\n",
    "    print(f\"Classes are balanced. Max-to-min count ratio is: {min_y_count/max_y_count}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c8688c6-9e33-4387-b2a4-bf00c200d239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 48\n",
      "Total features: 321\n",
      "Number of classes: 16\n",
      "Class distribution in full dataset:\n",
      "[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n"
     ]
    }
   ],
   "source": [
    "# STEP 0: Split X and y\n",
    "\n",
    "y =  df_new['Subject_ID_Y']\n",
    "X =  df_new.drop('Subject_ID_Y', axis='columns')\n",
    "\n",
    "print(f\"Total samples: {len(y)}\")\n",
    "print(f\"Total features: {X.shape[1]}\")\n",
    "print(f\"Number of classes: {len(y.unique())}\")\n",
    "print(f\"Class distribution in full dataset:\")\n",
    "print(y.value_counts().sort_index().to_list())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ef03859e-9489-44b9-9a01-33575f764646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== smoteenn ===\n",
      "y_train distribution: [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "k_neighbors: 1\n",
      "y_train.value_counts().min(): 2\n",
      "Only 2 samples per class - using RandomOverSampler instead of smoteenn\n",
      "oversampler used:  RandomOverSampler(random_state=42)\n",
      "Skipping cross-validation - only 2 samples per class\n",
      "Training on training set and evaluating on test set only...\n",
      "\n",
      "Test Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         1\n",
      "           1       0.00      0.00      0.00         1\n",
      "           2       0.00      0.00      0.00         1\n",
      "           3       0.20      1.00      0.33         1\n",
      "           4       1.00      1.00      1.00         1\n",
      "           5       1.00      1.00      1.00         1\n",
      "           6       0.33      1.00      0.50         1\n",
      "           7       1.00      1.00      1.00         1\n",
      "           8       0.00      0.00      0.00         1\n",
      "           9       0.00      0.00      0.00         1\n",
      "          10       0.00      0.00      0.00         1\n",
      "          11       1.00      1.00      1.00         1\n",
      "          12       1.00      1.00      1.00         1\n",
      "          13       0.00      0.00      0.00         1\n",
      "          14       0.50      1.00      0.67         1\n",
      "          15       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.50        16\n",
      "   macro avg       0.38      0.50      0.41        16\n",
      "weighted avg       0.38      0.50      0.41        16\n",
      "\n",
      "\n",
      "=== smotetomek ===\n",
      "y_train distribution: [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "k_neighbors: 1\n",
      "y_train.value_counts().min(): 2\n",
      "Only 2 samples per class - using RandomOverSampler instead of smotetomek\n",
      "oversampler used:  RandomOverSampler(random_state=42)\n",
      "Skipping cross-validation - only 2 samples per class\n",
      "Training on training set and evaluating on test set only...\n",
      "\n",
      "Test Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         1\n",
      "           1       0.00      0.00      0.00         1\n",
      "           2       0.00      0.00      0.00         1\n",
      "           3       0.20      1.00      0.33         1\n",
      "           4       1.00      1.00      1.00         1\n",
      "           5       1.00      1.00      1.00         1\n",
      "           6       0.33      1.00      0.50         1\n",
      "           7       1.00      1.00      1.00         1\n",
      "           8       0.00      0.00      0.00         1\n",
      "           9       0.00      0.00      0.00         1\n",
      "          10       0.00      0.00      0.00         1\n",
      "          11       1.00      1.00      1.00         1\n",
      "          12       1.00      1.00      1.00         1\n",
      "          13       0.00      0.00      0.00         1\n",
      "          14       0.50      1.00      0.67         1\n",
      "          15       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.50        16\n",
      "   macro avg       0.38      0.50      0.41        16\n",
      "weighted avg       0.38      0.50      0.41        16\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define oversampler TYPES\n",
    "oversampler_types = ['smoteenn', 'smotetomek']\n",
    "oversampler_count = len(oversampler_types)\n",
    "\n",
    "for key in oversampler_types:\n",
    "    print(f'\\n=== {key} ===')\n",
    "    \n",
    "    # Train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.33, random_state=42 )\n",
    "    print(\"y_train distribution:\", y_train.value_counts().sort_index().to_list())\n",
    "    \n",
    "    # Get sample counts in train data\n",
    "    n_samples = y_train.value_counts().min()\n",
    "    k_neighbors = max(1, min(3, n_samples - 1))\n",
    "    print(\"k_neighbors:\", k_neighbors)\n",
    "    print(\"y_train.value_counts().min():\", n_samples)\n",
    "    \n",
    "    # Choose appropriate oversampler based on sample size\n",
    "    if n_samples < 6:  # Too few samples for SMOTEENN/SMOTETomek\n",
    "        print(f\"Only {n_samples} samples per class - using RandomOverSampler instead of {key}\")\n",
    "        oversampler = RandomOverSampler(random_state=42)\n",
    "    else:\n",
    "        sampling_strategy = {cls: target_samples_per_class for cls in np.unique(y_train)}\n",
    "        \n",
    "        if key == 'smoteenn':\n",
    "            oversampler = SMOTEENN(\n",
    "                smote=SMOTE(random_state=42, sampling_strategy=sampling_strategy, k_neighbors=k_neighbors),\n",
    "                random_state=42,\n",
    "                n_jobs=-1\n",
    "            )\n",
    "        elif key == 'smotetomek':\n",
    "            oversampler = SMOTETomek(\n",
    "                smote=SMOTE(random_state=42, sampling_strategy=sampling_strategy, k_neighbors=k_neighbors),\n",
    "                random_state=42,\n",
    "                n_jobs=-1\n",
    "            )\n",
    "\n",
    "    print(\"oversampler used: \", oversampler)    \n",
    "    \n",
    "    # Pipeline with GaussianNB\n",
    "    max_k = min(10, X_train.shape[1], len(X_train) // 10)\n",
    "    pipeline = Pipeline([\n",
    "        ('oversample', oversampler),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('selector', SelectKBest(mutual_info_classif, k=max_k)),\n",
    "        ('model', GaussianNB())\n",
    "    ])\n",
    "    \n",
    "    try:\n",
    "        # Check if we have enough samples for cross-validation\n",
    "        if n_samples < 3:\n",
    "            print(f\"Skipping cross-validation - only {n_samples} samples per class\")\n",
    "            print(\"Training on training set and evaluating on test set only...\")\n",
    "            \n",
    "            # Just fit on training data and evaluate on test\n",
    "            pipeline.fit(X_train, y_train)\n",
    "            y_pred_test = pipeline.predict(X_test)\n",
    "            \n",
    "            print(\"\\nTest Report:\")\n",
    "            print(classification_report(y_test, y_pred_test, zero_division=0))\n",
    "        else:\n",
    "            # Cross-validation\n",
    "            n_splits = min(5, n_samples)\n",
    "            cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "            \n",
    "            y_pred_train = cross_val_predict(pipeline, X_train, y_train, cv=cv, n_jobs=-1)\n",
    "            print(\"\\nTraining CV Report:\")\n",
    "            print(classification_report(y_train, y_pred_train, zero_division=0))\n",
    "            \n",
    "            # Final evaluation\n",
    "            pipeline.fit(X_train, y_train)\n",
    "            y_pred_test = pipeline.predict(X_test)\n",
    "            print(\"\\nTest Report:\")\n",
    "            print(classification_report(y_test, y_pred_test, zero_division=0))\n",
    "\n",
    "        \n",
    "    except ValueError as e:\n",
    "        print(f\"Error with {key}: {e}\")\n",
    "        print(\"Debug info:\")\n",
    "        print(f\"  - X_train shape: {X_train.shape}\")\n",
    "        print(f\"  - y_train distribution: {y_train.value_counts().to_dict()}\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072d5a97-df9b-416a-900f-cf0b3cf70d77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "57be86fe-7f9e-4bd0-949d-43786d2df9e9",
   "metadata": {},
   "source": [
    "Conclusion from this analysis:\n",
    "- Terrible model performance: 0.00 recall/precision for many of the 16 classes\n",
    "- The model never predicts these classes where recall/precision is 0.0\n",
    "- Only 1 test sample/class: completely unreliable evaluation\n",
    "- Only 2 training samples/class - impossible to learn patterns. \n",
    "- With only 2 training samples per class, the model can't learn distinguishing features\n",
    "- Even after oversampling, there's not enough diversity in the synthetic samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103032aa-61ed-45a1-b029-04f2d69c5df0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
